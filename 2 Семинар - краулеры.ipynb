{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Семинар 2. Выкачиваем Интернет (продолжение).\n",
    "\n",
    "Эпиграф: *У меня есть дома интернет. Можно я его скачаю на флешку и с собой принесу?*",
    "\n",
    "## Введение\n",
    "\n",
    "На прошлом семинаре мы научились скачивать из Интернета HTML-код страницы с заданным адресом. Это здорово, но на практике обычно приходится выкачивать содержимое не одной страницы, а целого сайта или даже многих сайтов --- тысячи или миллионы страниц. Понятно, что перечислить все интересующие нас адреса вручную, чтобы выкачать их по отдельности, в таком случае не получится. На этом семинаре мы выясним, как можно выкачивать страницы оптом, а также научимся получше их чистить от ненужных вещей.\n",
    "\n",
    "Основной проблемой, которую нужно решить при выкачивании большого количества страниц, --- как узнать адреса всех этих страниц. Мы рассмотрим два подхода.\n",
    "\n",
    "*Первый подход* обычно применяется, когда нужно загрузить все страницы какого-нибудь крупного ресурса --- например, газеты или форума. Адреса страниц на таких сайтах нередко устроены довольно просто: начинаются они все одинаково, а заканчиваются разными числами. Если внимательно посмотреть на адреса нескольких произвольных страниц, можно довольно быстро выяснить, так ли это и каков допустимый диапазон номеров страниц. В этом случае закачка всех страниц будет представлять собой простой цикл, в котором будут перебираться все номера страниц из этого диапазона.\n",
    "\n",
    "*Второй подход* обычно применяется в **краулерах** --- программах, которые обходят какой-то фрагмент интернета, собирая информацию с разных сайтов. Краулерами, например, пользуются поисковые системы, чтобы индексировать содержимое сайтов. Краулер начинает работу с одной или нескольких страниц, адреса которых задаются вручную, а затем переходит по всем ссылкам из этих страниц. Каждый раз, когда краулер загружает очередную страницу, он находит на ней не только нужную ему информацию, но и все ссылки, которые добавляются в очередь. Важно при этом помнить, где краулер уже побывал, чтобы не переходить по нескольку раз на одни и те же страницы. В настоящих краулерах применяют и другие ухищрения, например, чтобы выяснить, по каким ссылкам лучше переходить сначала, но мы этого касаться не будем.\n",
    "\n",
    "\n",
    "## Напоминание\n",
    "\n",
    "Ссылки в HTML задаются тэгом `a`, а сам адрес находится в атрибуте `href`. Ссылка обязательно должна начинаться с протокола (`http://`); если это не так, это означает, что ссылка указывает на другую страницу на том же сайте. При поиске ссылок в HTML нужно помнить, что между тэгом (`a`) и атрибутом (`href`) могут находиться другие атрибуты и любое количество пробелов.\n",
    "\n",
    "## Пример\n",
    "\n",
    "Чтобы было не так скучно, в этот раз мы будем тренироваться не на русском сайте, а на албанском. Допустим, мы хотим скачать для корпуса все посты с http://www.forumishqiptar.com --- главного албанского форума. С ходу трудно разобраться, что там к чему, но если присмотреться, станет понятно, что перед нами список тем. Нажав на какую-нибудь тему, мы попадём на страницу со списком тредов, а нажав на тред, обнаружим страницу с постами. Это, видимо, и есть та страница нижнего уровня, которые мы хотим скачивать.\n",
    "\n",
    "Посмотрим на адреса нескольких таких страниц. Выглядят они примерно так:\n",
    "\n",
    "http://www.forumishqiptar.com/threads/79403-%C3%87far%C3%AB-%C3%ABsht%C3%AB-dashuria\n",
    "http://www.forumishqiptar.com/threads/41551-P%C3%ABrkufizimi-i-dashuris%C3%AB%21%21\n",
    "http://www.forumishqiptar.com/threads/160424-Mos-luaj-me-dashurin\n",
    "\n",
    "Видно, что у них общее начало и у каждой страницы есть свой номер. Номера легко можно было бы перебрать, но кроме номера, в адресе стоит ещё и заголовок, который нам заранее неизвестен. Одним из решений было бы загрузить сначала страницы с темами и найти в них полные ссылки на треды. Однако мы применим хитрость, которая срабатывает в 90% случаев вроде этого. Попробуем взять адрес какой-нибудь страницы, вручную убрать в нём всё после числа и ввести это в адресную строку браузера. Вуаля! Всё работает и без заголовка (точнее, происходит автоматическое перенаправление). Это значит, что мы можем пользоваться перебором номеров. Для этого нам достаточно узнать диапазон --- номера самой первой и самой последней (по времени) страницы. Это мы оставляем в качестве упражнения читателю. В результате должно получиться что-то такое:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at http://www.forumishqiptar.com/threads/160403\n",
      "Error at http://www.forumishqiptar.com/threads/160407\n",
      "Error at http://www.forumishqiptar.com/threads/160408\n",
      "Error at http://www.forumishqiptar.com/threads/160411\n",
      "Error at http://www.forumishqiptar.com/threads/160412\n",
      "Error at http://www.forumishqiptar.com/threads/160413\n",
      "Error at http://www.forumishqiptar.com/threads/160415\n",
      "Error at http://www.forumishqiptar.com/threads/160418\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "def download_page(pageUrl):\n",
    "    try:\n",
    "        page = urllib.request.urlopen(pageUrl)\n",
    "        text = page.read().decode('ISO-8859-1')\n",
    "    except:\n",
    "        print('Error at', pageUrl)\n",
    "        return\n",
    "    # do something with the downloaded text\n",
    "\n",
    "commonUrl = 'http://www.forumishqiptar.com/threads/'\n",
    "for i in range(160400, 160425):\n",
    "    pageUrl = commonUrl + str(i)\n",
    "    download_page(pageUrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В функции, отвечающей за загрузку, мы поместили собственно загрузку HTML в блок `try-except`. Это было сделано потому, что зачастую не всем номерам из допустимого диапазона соответствуют реальные страницы. Если страницы с таким адресом не существует, функция `urlopen` вызовет ошибку, которая благодаря `try-except` не вызовет падения всей программы.\n",
    "\n",
    "## Важный комментарий\n",
    "\n",
    "Когда Ваша программа выкачивает много страниц сразу, она создаёт нагрузку на сервер выкачиваемого сайта --- и эта нагрузка намного больше, чем нагрузка от обычного посещения сайта людьми. Если Вы выкачиваете содержимое крупного сайта с одного компьютера, то ничего страшного в этом, скорее всего, нет. Но если это не какой-то крупный ресурс, который владеет мощными серверами, и тем более если страницы с него скачивают несколько человек одновременно, это может создать реальные проблемы владельцам выкачиваемого ресурса. Поэтому, во-первых, нужно всегда выяснять, не доступно ли всё содержимое нужного Вам ресурса по отдельной ссылке (например, так обстоит дело с Википедией), а во-вторых, ставить между обращениями к серверу искуственный временной интервал хотя бы в пару секунд:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как почистить текст\n",
    "\n",
    "В html странице, конечно, всегда много тэгов, скриптов и комментариев. А нам обычно бывает нужен только текст.\n",
    "Чтобы вытащить из html только текст, можно воспользоваться регулярными выражениями:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "html_content = '<html>....</html>'  # тут какой-то html\n",
    "\n",
    "regTag = re.compile('<.*?>', flags=re.U | re.DOTALL)  # это рег. выражение находит все тэги\n",
    "regScript = re.compile('<script>.*?</script>', flags=re.U | re.DOTALL) # все скрипты\n",
    "regComment = re.compile('<!--.*?-->', flags=re.U | re.DOTALL)  # все комментарии\n",
    "\n",
    "# а дальше заменяем ненужные куски на пустую строку\n",
    "clean_t = regScript.sub(\"\", t)\n",
    "clean_t = regComment.sub(\"\", clean_t)\n",
    "clean_t = regTag.sub(\"\", clean_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно не забывать, что для отображения на html-странице символов, которых нет на клавиатуре, применяются специальные последовательности символов, начинающиеся с амперсанда (&) и заканчивающиеся точкой с запятой (;). Чтобы получить текст не с такими последовательностями, а с нормальными символами, используется специальная функция в питоне `unescape`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Петя & Вася\n"
     ]
    }
   ],
   "source": [
    "# если у вас Python3.4+\n",
    "import html\n",
    "test_string = 'Петя &amp; Вася'\n",
    "print( html.unescape(test_string) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Специальные символы: \" « » ♠ ♥ ♣ ♦ и так далее\n"
     ]
    }
   ],
   "source": [
    "print( html.unescape('Специальные символы: &quot; &laquo; &raquo; &spades; &hearts; &clubs; &diams; и так далее') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# если у вас Python3.3 или более ранняя версия \n",
    "import html.parser    \n",
    "html.parser.HTMLParser().unescape('Петя &amp; Вася')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание\n",
    "\n",
    "Вообще-то в длинных постах бывает по многу страниц, например, как тут:\n",
    "http://www.forumishqiptar.com/threads/79403\n",
    "Нужно написать код, который умеет скачивать все страницы треда, а не только первую."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
